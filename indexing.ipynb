{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## project configurations\n",
    "from elasticsearch import Elasticsearch\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import os\n",
    "\n",
    "files = os.listdir('archive/sgm')\n",
    "es = Elasticsearch()\n",
    "index_name = \"esdocs\"\n",
    "files_limit = len(files) # len(files) or 1 for testing\n",
    "bulk_size = 20\n",
    "index_limit = files_limit*1000 # of bulk size, less than files_limit*1000\n",
    "max_geo_limit = 2 # max number of geo to collect from API (Time consuming)\n",
    "date_format = \"%d-%b-%Y %H:%M:%S\" # date format in sgm files\n",
    "python_date_format = \"%Y-%m-%d %H:%M:%S\" # date format in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init status: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'esdocs'}\n",
      "test status: {'esdocs': {'aliases': {}, 'mappings': {'properties': {'author': {'type': 'nested', 'properties': {'first_name': {'type': 'text'}, 'last_name': {'type': 'text'}}}, 'content': {'type': 'text'}, 'date': {'type': 'date', 'format': 'dd-MMM-yyyy HH:mm:ss'}, 'geo_references': {'type': 'nested', 'properties': {'geo_name': {'type': 'text'}, 'geopoint': {'type': 'geo_point'}, 'name': {'type': 'keyword'}}}, 'geopoint': {'type': 'geo_point'}, 'id': {'type': 'text'}, 'people': {'type': 'keyword'}, 'temporal_expressions': {'type': 'nested', 'properties': {'text': {'type': 'text'}}}, 'title': {'type': 'text', 'analyzer': 'autocomplete', 'search_analyzer': 'standard'}}}, 'settings': {'index': {'routing': {'allocation': {'include': {'_tier_preference': 'data_content'}}}, 'number_of_shards': '1', 'provided_name': 'esdocs', 'creation_date': '1704207151546', 'analysis': {'analyzer': {'autocomplete': {'filter': ['lowercase', 'stop', 'porter_stem'], 'char_filter': ['html_strip'], 'tokenizer': 'autocomplete'}}, 'tokenizer': {'autocomplete': {'token_chars': ['letter'], 'min_gram': '3', 'type': 'edge_ngram', 'max_gram': '10'}}}, 'number_of_replicas': '1', 'uuid': 'MJ2afQnBTryvrMo_DxVpjA', 'version': {'created': '8100499'}}}}}\n"
     ]
    }
   ],
   "source": [
    "## ElasticSearch Setup\n",
    "\n",
    "# This test is done during development only. \n",
    "if es.indices.exists(index_name):\n",
    "  es.indices.delete(index=index_name)\n",
    "\n",
    "#? creating index\n",
    "with open('elastic.config.json', 'r') as file:\n",
    "  es_config = json.load(file)\n",
    "  \n",
    "#? getting status of index creation\n",
    "es_init = es.indices.create(index=index_name, ignore=400, body=es_config)\n",
    "es_test = es.indices.get(index=index_name)\n",
    "print(f'init status: {es_init}\\ntest status: {es_test}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data processing functions\n",
    "def author_handler(text) -> List[str]:\n",
    "  author_fname, author_lname = None, None\n",
    "  if text:\n",
    "    authors = text.split(' ')\n",
    "    if len(authors) >= 7:\n",
    "      author_fname = authors[5]\n",
    "      author_lname = authors[6].split(',')[0]\n",
    "  return [author_fname, author_lname]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full-featured documents: 88.25%\n"
     ]
    }
   ],
   "source": [
    "## Data Preprocessing\n",
    "from bs4 import BeautifulSoup\n",
    "import codecs\n",
    "\n",
    "reuters_dict = {}\n",
    "fails = 0\n",
    "for file in files[:files_limit]:\n",
    "  with codecs.open(f'archive/sgm/{file}', 'r', 'latin-1') as f:\n",
    "    soup = BeautifulSoup(f.read(), 'lxml')\n",
    "    reuters = soup.find_all('reuters')\n",
    "    for i in reuters:\n",
    "      try:\n",
    "        id = i.get('newid')\n",
    "        date = i.find('date').get_text()\n",
    "        places = [tag.get_text() for tag in i.find('places').find_all('d')]\n",
    "        people = i.find('people').text\n",
    "        authors = i.find('author')\n",
    "        author_fname, author_lname = None, None\n",
    "        if authors:\n",
    "          authors = authors.text.split(' ') \n",
    "          author_fname = authors[5]\n",
    "          author_lname = authors[6].split(',')[0]\n",
    "        else:\n",
    "          authors = None\n",
    "        title = i.find('text').find('title').get_text()\n",
    "        dateline = i.find('dateline').get_text()\n",
    "        body = i.find('content').get_text()\n",
    "      except:\n",
    "        fails += 1\n",
    "        pass\n",
    "      reuters_dict[id] = {\n",
    "        'places': places,\n",
    "        'people': people,\n",
    "        'author': [author_fname, author_lname],\n",
    "        'title': title,\n",
    "        'dateline': dateline,\n",
    "        'date': date,\n",
    "        'body': body\n",
    "        }\n",
    "\n",
    "#* Finding Total Indexed Documents (some documents don't all fields)\n",
    "all_docs = len(reuters_dict)\n",
    "print(f'Full-featured documents: {round((all_docs-fails)/all_docs*100, 2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'en_core_web_sm' (3.5.0) was trained with spaCy v3.5.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'places': ['el-salvador', 'usa', 'uruguay'],\n",
       " 'people': '',\n",
       " 'author': [None, None],\n",
       " 'title': 'BAHIA COCOA REVIEW',\n",
       " 'dateline': '    SALVADOR, Feb 26 - ',\n",
       " 'date': '26-FEB-1987 15:01:01.79',\n",
       " 'body': 'Showers continued throughout the week in\\nthe Bahia cocoa zone, alleviating the drought since early\\nJanuary and improving prospects for the coming temporao,\\nalthough normal humidity levels have not been restored,\\nComissaria Smith said in its weekly review.\\n    The dry period means the temporao will be late this year.\\n    Arrivals for the week ended February 22 were 155,221 bags\\nof 60 kilos making a cumulative total for the season of 5.93\\nmln against 5.81 at the same stage last year. Again it seems\\nthat cocoa delivered earlier on consignment was included in the\\narrivals figures.\\n    Comissaria Smith said there is still some doubt as to how\\nmuch old crop cocoa is still available as harvesting has\\npractically come to an end. With total Bahia crop estimates\\naround 6.4 mln bags and sales standing at almost 6.2 mln there\\nare a few hundred thousand bags still in the hands of farmers,\\nmiddlemen, exporters and processors.\\n    There are doubts as to how much of this cocoa would be fit\\nfor export as shippers are now experiencing dificulties in\\nobtaining +Bahia superior+ certificates.\\n    In view of the lower quality over recent weeks farmers have\\nsold a good part of their cocoa held on consignment.\\n    Comissaria Smith said spot bean prices rose to 340 to 350\\ncruzados per arroba of 15 kilos.\\n    Bean shippers were reluctant to offer nearby shipment and\\nonly limited sales were booked for March shipment at 1,750 to\\n1,780 dlrs per tonne to ports to be named.\\n    New crop sales were also light and all to open ports with\\nJune/July going at 1,850 and 1,880 dlrs and at 35 and 45 dlrs\\nunder New York july, Aug/Sept at 1,870, 1,875 and 1,880 dlrs\\nper tonne FOB.\\n    Routine sales of butter were made. March/April sold at\\n4,340, 4,345 and 4,350 dlrs.\\n    April/May butter went at 2.27 times New York May, June/July\\nat 4,400 and 4,415 dlrs, Aug/Sept at 4,351 to 4,450 dlrs and at\\n2.27 and 2.28 times New York Sept and Oct/Dec at 4,480 dlrs and\\n2.27 times New York Dec, Comissaria Smith said.\\n    Destinations were the U.S., Covertible currency areas,\\nUruguay and open ports.\\n    Cake sales were registered at 785 to 995 dlrs for\\nMarch/April, 785 dlrs for May, 753 dlrs for Aug and 0.39 times\\nNew York Dec for Oct/Dec.\\n    Buyers were the U.S., Argentina, Uruguay and convertible\\ncurrency areas.\\n    Liquor sales were limited with March/April selling at 2,325\\nand 2,380 dlrs, June/July at 2,375 dlrs and at 1.25 times New\\nYork July, Aug/Sept at 2,400 dlrs and at 1.25 times New York\\nSept and Oct/Dec at 1.25 times New York Dec, Comissaria Smith\\nsaid.\\n    Total Bahia sales are currently estimated at 6.13 mln bags\\nagainst the 1986/87 crop and 1.06 mln bags against the 1987/88\\ncrop.\\n    Final figures for the period to February 28 are expected to\\nbe published by the Brazilian Cocoa Trade Commission after\\ncarnival which ends midday on February 27.\\n Reuter\\n'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## data extraction definitions\n",
    "from datetime import datetime\n",
    "import dateparser\n",
    "from collections import Counter\n",
    "\n",
    "import spacy\n",
    "from geopy.geocoders import Nominatim\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "geo_locator = Nominatim(user_agent=\"es_indexer app\")\n",
    "\n",
    "loc_types = ['GPE', 'LOC', 'FAC']\n",
    "ner_types_datetime = ['DATE', 'TIME']\n",
    "\n",
    "#* taking a sample\n",
    "reuters_dict['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '1',\n",
       " 'title': 'Japan February external reserves record 51.73 billion dlrs (January 51.46 billion)\\n',\n",
       " 'date': '02-Mar-1987 02:48:11',\n",
       " 'geopoint': {'lon': 139.2394179, 'lat': 36.5748441},\n",
       " 'geo_references': [{'name': 'JAPAN',\n",
       "   'geo_name': '日本',\n",
       "   'geopoint': {'lon': 139.2394179, 'lat': 36.5748441}}],\n",
       " 'expressions': [0, 2921, -366, 2010, -1],\n",
       " 'content': 'Thai Airways International plans to\\nexpand its fleet to 58 from 30 aircraft by 1995, company\\nofficials said.\\n    Thamnoon Wanglee, vice-president for finance, told a\\nweekend marketing conference Thai would finance the expansion\\nby borrowing, but he did not give details.\\n    He said the airline planned to reduce its yen borrowing to\\n36.4 pct of overall debt by September 1992. It is currently\\n64.3 pct of overall debt.\\n    He said dollar borrowing should rise to 56.2 pct of overall\\ndebt in the same period, compared to 15.7 pct now.\\n    Other company officials said the state-owned airline had no\\nplans to go private. They said the airline is studying a\\ngovernment proposal for it to merge with Thai Airways Company,\\nthe state-owned domestic carrier.\\n    A report presented to the conference showed the airline\\nexpects passenger sales revenue to be 13 pct higher in 1987\\nthan in 1986. This follows a 20 pct jump in passenger sales\\nrevenue in the past four months.\\n    Executive vice president Chatrachai Bunya-ananta said the\\ncurrent expansion of Bangkok airport would be completed this\\nyear.\\n REUTER\\n',\n",
       " 'people': '',\n",
       " 'author': {'first_name': None, 'last_name': None}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## data extraction functions\n",
    "def date_extractor(date) -> str:\n",
    "  \"\"\"\n",
    "  #### Args\n",
    "    date (str): The date holding format 'dd-MMM-yyyy HH:mm:ss.sss'.\n",
    "  #### Returns\n",
    "    str: The extracted date in the format 'dd-MMM-yyyy HH:mm:ss'.\n",
    "  \"\"\"\n",
    "  datesplit = date.split('.')[0]\n",
    "  dt = datetime.strptime(datesplit, date_format)\n",
    "  new_dt = dt.strftime(date_format)\n",
    "  return new_dt\n",
    "\n",
    "def ner_extractor(doc, doc_date) -> List[int]:\n",
    "  \"\"\"\n",
    "  Extracts named entities related to datetime from the given document.\n",
    "\n",
    "  #### Args:\n",
    "    doc (Dict): The document to extract named entities from.\n",
    "    doc_date (str): The date of the document in the format specified by `date_format`.\n",
    "\n",
    "  #### Returns:\n",
    "    List[int]: A list of integers representing the number of days between the named entity and the document date.\n",
    "  \"\"\"\n",
    "  body_text = nlp(doc['body'])\n",
    "  expressions = []\n",
    "  for ent in body_text.ents:\n",
    "    if ent.label_ in ner_types_datetime:\n",
    "      #* getting the datetime object, then parsing the relative date \n",
    "      doc_datetime = datetime.strptime(doc_date, date_format)\n",
    "      newdate = dateparser.parse(ent.text, settings={'RELATIVE_BASE': doc_datetime})\n",
    "      if newdate:\n",
    "        newdate_datetime = datetime.strptime(str(newdate)[0:19], python_date_format)\n",
    "        days = (newdate_datetime - doc_datetime).days\n",
    "        expressions.append(days)\n",
    "  expressions = list(set(expressions))\n",
    "  return expressions\n",
    "\n",
    "def get_locations(doc) -> List[Dict]:\n",
    "  \"\"\"\n",
    "  - Extracts and geocodes the top geo locations from a document.\n",
    "  #### Args\n",
    "    doc (dict): The document containing `title`, `body`, `dateline`, and `places` fields.\n",
    "  #### Returns\n",
    "    list: A list of dictionaries representing the top geo locations. Each dictionary contains the following keys:\n",
    "      - `name`: The name of the location.\n",
    "      - `geo_name`: The geocoded name of the location.\n",
    "      - `geopoint`: A dictionary with 'lon' and 'lat' keys representing the longitude and latitude of the location.\n",
    "  \"\"\"\n",
    "  title_text = nlp(doc['title'].upper())\n",
    "  title_ents = [i.text for i in title_text.ents if i.label_ in loc_types]\n",
    "  \n",
    "  body_text = nlp(doc['body'].replace('\\n', ' ').upper())\n",
    "  body_ents = [i.text for i in body_text.ents if i.label_ in loc_types]\n",
    "  \n",
    "  dateline_text = nlp(doc['dateline'].upper())\n",
    "  dateline_ents = [i.text for i in dateline_text.ents if i.label_ in loc_types]\n",
    "\n",
    "  places_ents = ' '.join(doc['places']).upper().split(' ')\n",
    "  all_entities = list(body_ents + dateline_ents + places_ents + title_ents)\n",
    "  \n",
    "  #* counting and sorting the entities by frequency\n",
    "  location_counts = Counter(all_entities)\n",
    "  location_counts = dict(sorted(location_counts.items(), key=lambda item: item[1], reverse=True))\n",
    "  \n",
    "  all_locs = []\n",
    "  idx = 1\n",
    "  for key, value in location_counts.items():\n",
    "    if idx > max_geo_limit and key:\n",
    "      all_locs.append({\n",
    "        'name': key,\n",
    "      })\n",
    "    else:\n",
    "      try:\n",
    "        location = geo_locator.geocode(key)\n",
    "      except:\n",
    "        pass\n",
    "      if location:\n",
    "        all_locs.append({\n",
    "          'name': key,\n",
    "          'geo_name': location.address,\n",
    "          'geopoint': {\n",
    "            'lon': location.longitude,\n",
    "            'lat': location.latitude\n",
    "          }\n",
    "        })\n",
    "        idx += 1\n",
    "  return all_locs\n",
    "\n",
    "def get_item(key, object) -> dict:\n",
    "  \"\"\"\n",
    "  - Get item details from the given object.\n",
    "  #### Args\n",
    "    key (str): The key of the item.\n",
    "    object (dict): The object containing item details.\n",
    "  #### Returns\n",
    "    dict: A dictionary containing the item details.\n",
    "  \"\"\"\n",
    "  id = key\n",
    "  date = date_extractor(object['date'])\n",
    "  title = object['title']\n",
    "  content = object['body']\n",
    "  people = object['people']\n",
    "  #* These are too slow to process\n",
    "  all_locations = get_locations(object)\n",
    "  try:\n",
    "    geopoint = {\n",
    "    'lon': all_locations[0]['geopoint']['lon'],\n",
    "    'lat': all_locations[0]['geopoint']['lat'],\n",
    "  }\n",
    "  except:\n",
    "    geopoint = {}\n",
    "  geo_ref = all_locations\n",
    "  exprs = ner_extractor(object, date)\n",
    "  item = {\n",
    "    'id': id,\n",
    "    'title': title,\n",
    "    'date': date,\n",
    "    'geopoint': geopoint,\n",
    "    'geo_references': geo_ref,\n",
    "    'expressions': exprs,\n",
    "    'content': content,\n",
    "    'people': people,\n",
    "    'author': {\n",
    "      'first_name': object['author'][0],\n",
    "      'last_name': object['author'][1]\n",
    "    }\n",
    "  }\n",
    "  return item\n",
    "\n",
    "get_item('1',reuters_dict['283']) #? testing geopy service for a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## indexing to Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "actions = []\n",
    "for idx, (key, value) in enumerate(reuters_dict.items()):\n",
    "  if idx > index_limit:\n",
    "    break\n",
    "  try:\n",
    "    actions.append({\n",
    "      \"_index\": index_name,\"_id\": int(key),\n",
    "      \"_source\": get_item(key, value)\n",
    "    })\n",
    "    if idx % bulk_size == 0 and idx > 0:\n",
    "      print(f\"Processing bulk: {idx} records\")\n",
    "      helpers.bulk(es, actions)\n",
    "      actions = []\n",
    "  except Exception as e:\n",
    "    print(f\"Error processing at key: {key}\")\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
